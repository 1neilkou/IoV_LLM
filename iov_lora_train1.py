import os
import torch
import json
from datasets import Dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer, 
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model, TaskType

# 1. ç¯å¢ƒé…ç½®
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
os.environ["TRANSFORMERS_OFFLINE"] = "1"
os.environ["HF_DATASETS_OFFLINE"] = "1"

# 2. è·¯å¾„è®¾ç½®
model_id = "./models/Qwen2.5-7B-Instruct"
dataset_path = "./data/iov_train_data.jsonl"
output_dir = "./output/iov_qwen_lora"

def process_func(example, tokenizer):
    """
    é’ˆå¯¹ Qwen æ ¼å¼æ„å»ºå¾®è°ƒ Prompt Template
    """
    MAX_LENGTH = 512
    input_ids, labels = [], []
    
    # æ„å»ºè½¦è”ç½‘é¢†åŸŸçš„æŒ‡ä»¤æ ¼å¼
    instruction = tokenizer(
        f"<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªè½¦è”ç½‘ä¸“å®¶ï¼Œè¯·æ ¹æ®æä¾›çš„è®ºæ–‡å†…å®¹å›ç­”é—®é¢˜ã€‚<|im_end|>\n"
        f"<|im_start|>user\n{example['instruction']}<|im_end|>\n"
        f"<|im_start|>assistant\n",
        add_special_tokens=False
    )
    response = tokenizer(f"{example['output']}<|im_end|>\n", add_special_tokens=False)
    
    input_ids = instruction["input_ids"] + response["input_ids"]
    # æ ‡ç­¾ä¸­ï¼ŒæŒ‡ä»¤éƒ¨åˆ†ç”¨ -100 å¿½ç•¥ï¼Œåªè®¡ç®—å›ç­”éƒ¨åˆ†çš„ Loss
    labels = [-100] * len(instruction["input_ids"]) + response["input_ids"]
    
    if len(input_ids) > MAX_LENGTH:
        input_ids = input_ids[:MAX_LENGTH]
        labels = labels[:MAX_LENGTH]
        
    return {
        "input_ids": input_ids,
        "labels": labels
    }

def main():
    print("ğŸš€ æ­£åœ¨åˆå§‹åŒ– 5090 å®éªŒç¯å¢ƒ...")

    # åŠ è½½ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token

    # åŠ è½½æ•°æ®é›†
    with open(dataset_path, 'r', encoding='utf-8') as f:
        data = [json.loads(line) for line in f]
    dataset = Dataset.from_list(data)
    tokenized_id = dataset.map(lambda x: process_func(x, tokenizer), remove_columns=dataset.column_names)

    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_id, 
        torch_dtype=torch.bfloat16, 
        device_map="auto"
    )
    model.enable_input_require_grads() # å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹å¿…éœ€

    # LoRA é…ç½®
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,   
        r=16,
        lora_alpha=32,
        lora_dropout=0.1,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"] # è¦†ç›–å…¨é‡çº¿æ€§å±‚æå‡æ•ˆæœ
    )

    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    # è®­ç»ƒå‚æ•°è®¾ç½® - é’ˆå¯¹ 5090 (32GB) ä¼˜åŒ–
    args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=4,        # 5090 æ˜¾å­˜å……è£•ï¼Œå¯è®¾ä¸º 4-8
        gradient_accumulation_steps=4,        # ç­‰æ•ˆ batch_size = 16
        logging_steps=10,
        num_train_epochs=3,
        save_steps=100,
        learning_rate=1e-4,
        save_on_each_node=True,
        gradient_checkpointing=True,          # è¿›ä¸€æ­¥èŠ‚çœæ˜¾å­˜
        bf16=True,                            # 5090 å¿…é¡»å¼€å¯ bf16
        report_to="none"
    )

    # å¯åŠ¨ Trainer
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=tokenized_id,
        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),
    )

    print("\nâœ… LoRA å‡†å¤‡å°±ç»ªï¼Œå½“å‰æ˜¾å­˜å ç”¨ï¼š{:.2f} GB".format(torch.cuda.memory_allocated() / 1024**3))
    print("å¼€å§‹å¾®è°ƒè½¦è”ç½‘å¤§è„‘...")
    
    trainer.train()
    
    # ä¿å­˜ç»“æœ
    trainer.save_model(output_dir)
    print(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜è‡³: {output_dir}")

if __name__ == "__main__":
    main()