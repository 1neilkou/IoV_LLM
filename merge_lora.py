import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import os

# é…ç½®è·¯å¾„
base_model_path = "./models/Qwen2.5-7B-Instruct"
adapter_path = "./output/iov_qwen_lora"
save_path = "./models/Qwen2.5-7B-IoV-Final"

print(f"æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
tokenizer = AutoTokenizer.from_pretrained(base_model_path)
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    torch_dtype=torch.bfloat16,
    device_map="cpu", # åˆå¹¶å»ºè®®åœ¨å†…å­˜ä¸­è¿›è¡Œï¼Œé˜²æ­¢æ˜¾å­˜æº¢å‡º
    trust_remote_code=True
)

print(f"æ­£åœ¨åŠ è½½ LoRA æƒé‡: {adapter_path}")
model = PeftModel.from_pretrained(base_model, adapter_path)

print("æ­£åœ¨è¿›è¡Œæƒé‡ç†”ç„Š (Merging)...")
# merge_and_unload ä¼šå°† LoRA æƒé‡åˆå¹¶è¿›ä¸»æ¨¡å‹å¹¶å¸è½½ PEFT ç»“æ„
model = model.merge_and_unload()

print(f"æ­£åœ¨ä¿å­˜å®Œæ•´æ¨¡å‹è‡³: {save_path}")
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

print("ğŸ‰ åˆå¹¶å®Œæˆï¼ä½ ç°åœ¨æ‹¥æœ‰äº†ä¸€ä¸ªç‹¬ç«‹çš„ 15GB è½¦è”ç½‘ä¸“å®¶æ¨¡å‹ã€‚")